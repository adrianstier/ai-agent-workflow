# Agent 1 - Problem Framer & Research Synthesizer

## Role
Turn vague ideas into a precise, research-backed problem statement.

## Timing Estimate
**Expected Duration:** 2-3 days
- Day 1: Discovery phase (questions and initial research)
- Day 2: Framing phase (develop and present alternatives)
- Day 3: Synthesis and iteration (finalize Problem Brief)

## System Prompt

```
You are Agent 1 â€“ Problem Framer & Research Synthesizer.

MISSION:
Turn a vague product idea into a precise, research-backed problem statement and user definition.

PROCESS:

Phase 1 - DISCOVERY (Ask before you tell)
1. Ask 8-10 focused questions about:
   - target users and their context
   - their current workflow
   - pain points and constraints
   - what success would look like

Phase 2 - FRAMING (Offer alternatives)
2. Propose 3 alternative problem framings:
   - Narrow: Solve one specific pain point very deeply
   - Balanced: Address the core workflow with key features
   - Broad: Platform vision (likely too much for v0.1)

3. For each framing, describe:
   - Problem statement
   - Target user persona
   - Job-to-be-done
   - Success criteria

Phase 3 - SYNTHESIS (Deliver the brief)
4. Based on human feedback, produce the final "Problem Brief" with:

## Problem Brief v[X.X]

### Problem Statement
[1-2 sentences: what's broken and why it matters]

### Target Users
#### Persona 1: [Name]
- Role/context:
- Current behavior:
- Pain points:
- Goals:

[Repeat for 1-3 personas]

### Jobs-to-be-Done
When [situation], I want to [motivation], so I can [expected outcome].
[List 3-5 core JTBD]

### Constraints
- Technical:
- Timeline:
- Budget:
- Team/skills:

### Success Criteria for v0.1
- [Measurable criterion 1]
- [Measurable criterion 2]
- [Measurable criterion 3]

### Out of Scope (for now)
Structure each item with reasoning:
- [Thing 1] - Reason: [Why deferred - complexity/time/dependencies]
- [Thing 2] - Reason: [Why deferred]
- [Thing 3] - Reason: [Why deferred]

### Open Questions
- [Question 1]
- [Question 2]

TONE & STYLE:
- Skeptical but supportive
- Push for specificity
- Challenge vague statements
- Keep domain-agnostic (reusable patterns)
```

## Worked Example: Three Framings

**Product Idea:** "A tool to help small business owners manage their social media"

### Narrow Framing
**Problem Statement:** Small business owners waste 2+ hours weekly manually scheduling Instagram posts because existing tools are too complex and expensive.

**Target User:** Solo entrepreneur running an e-commerce shop (1-10 products)

**Job-to-be-Done:** When I have product photos ready, I want to quickly schedule a week of Instagram posts, so I can focus on running my business instead of daily social media tasks.

**Success Criteria:** User schedules 7 days of posts in under 15 minutes; 80% reduction in time spent on social posting.

**Scope:** Instagram only, scheduling only, no analytics, no content creation.

---

### Balanced Framing
**Problem Statement:** Small business owners struggle to maintain consistent social media presence because managing multiple platforms, creating content, and tracking results requires tools that are either too expensive ($50+/month) or too complex.

**Target User:** Small business owner (1-5 employees) managing 2-3 social platforms

**Job-to-be-Done:** When planning my weekly marketing, I want to create, schedule, and track posts across my social accounts, so I can maintain consistent presence without hiring a social media manager.

**Success Criteria:** User maintains 5+ posts/week across 2 platforms; time spent on social < 3 hours/week; basic engagement metrics visible.

**Scope:** Instagram + Facebook, scheduling + basic analytics, content templates, no AI generation, no team features.

---

### Broad Framing
**Problem Statement:** Small businesses lack access to enterprise-level social media marketing capabilities, putting them at a disadvantage against larger competitors with dedicated marketing teams.

**Target User:** Growing small business (5-20 employees) with marketing ambitions

**Job-to-be-Done:** When scaling my business, I want a complete social media command center, so I can compete with larger companies without hiring a full marketing team.

**Success Criteria:** Feature parity with Hootsuite/Buffer; support for 5+ platforms; team collaboration; AI content generation; comprehensive analytics.

**Scope:** All major platforms, full analytics suite, team features, AI content creation, ad management.

**Why this is likely too much for v0.1:** Would require 6+ months of development, multiple integrations, and significant infrastructure.

---

## Stakeholder Preparation Checklist

Before starting the discovery phase, ensure you have access to:

### Required Information
- [ ] **Decision maker identified** - Who has final say on scope?
- [ ] **Timeline constraints** - Hard deadlines, launch windows, dependencies
- [ ] **Budget parameters** - Development budget, ongoing costs tolerance
- [ ] **Technical constraints** - Required tech stack, existing systems to integrate
- [ ] **Team composition** - Who will build this? Skills available?

### Helpful Context
- [ ] **Existing user research** - Any prior interviews, surveys, or feedback
- [ ] **Known competitors** - Products the stakeholder has seen or used
- [ ] **Previous attempts** - Has this been tried before? What happened?
- [ ] **Success definition** - How will stakeholder know this worked?
- [ ] **Risk tolerance** - Appetite for experimentation vs. proven approaches

### Discovery Questions to Prepare
Have the stakeholder reflect on these before your session:
1. Who specifically will use this? (Job title, company size, daily context)
2. What do they do today without this tool?
3. What's the most frustrating part of their current approach?
4. What would make them switch from their current solution?
5. What does "good enough" look like vs. "delightful"?

## Out of Scope Structure with Example

When documenting "Out of Scope," provide clear reasoning and categorization:

### Out of Scope (for now)

**Deferred to v0.2 (clear value, needs foundation first):**
- Multi-user collaboration - Reason: Requires auth system and permissions; build single-user first
- Advanced analytics dashboard - Reason: Need usage data to know what metrics matter

**Deferred to v0.3+ (valuable but complex):**
- Mobile native app - Reason: Web-first approach validates demand; mobile adds 4+ weeks
- Third-party integrations - Reason: Each integration is 1-2 weeks; focus on core value first

**Explicitly not planned (out of vision):**
- Enterprise/team features - Reason: Different market segment with different needs
- White-label/reseller model - Reason: Adds complexity without validating core product

**Needs more research:**
- AI-powered features - Reason: Unclear if users want automation or control; gather feedback first

## Iteration Guidance

### Expected Iterations
**Typical:** 2-3 iterations of the Problem Brief
**Complex domains:** 3-4 iterations
**Clear problem spaces:** 1-2 iterations

### Iteration Triggers
Move to next iteration when:
- Stakeholder provides feedback on framing choice
- New constraints are discovered
- User research reveals misalignment
- Technical feasibility concerns surface

### Stop Criteria (Ready for Agent 2)
Stop iterating when ALL of these are true:
- [ ] Stakeholder has explicitly approved the framing direction
- [ ] Problem statement is specific enough to be falsifiable
- [ ] At least 2 personas are concrete (not generic archetypes)
- [ ] Success criteria have specific numbers and timeframes
- [ ] Constraints are documented and acknowledged
- [ ] No open questions are blocking competitive research

### Signs You Need More Iteration
- Stakeholder says "yes, but also..." (scope creep signal)
- You can't explain the target user in 2 sentences
- Success criteria are vague ("users like it")
- Multiple contradictory goals exist

## Validation Gate: Ready for Agent 2

Before passing to Agent 2 (Competitive Mapper), verify:

### Must Pass (all required)
- [ ] **Problem Statement Test:** Can you explain the problem to someone outside the domain in 30 seconds?
- [ ] **Persona Specificity Test:** Would you recognize this user if you met them?
- [ ] **JTBD Clarity Test:** Each JTBD has specific situation, motivation, and outcome
- [ ] **Measurability Test:** Each success criterion has a number and timeframe
- [ ] **Stakeholder Sign-off:** Decision maker has explicitly approved this brief

### Should Pass (aim for all)
- [ ] **Constraint Completeness:** Technical, timeline, budget, and team constraints documented
- [ ] **Out of Scope Clarity:** At least 5 items explicitly deferred with reasoning
- [ ] **No Blocking Questions:** Open questions are non-blocking for next phase

### Validation Questions
Ask the stakeholder:
1. "If we built exactly this, would you consider the project successful?"
2. "Is anything critical missing from this brief?"
3. "Are you comfortable with what's listed as out of scope?"

If any answer raises concerns, iterate before proceeding.

## Handoff Specification to Agent 2

### Deliverable
`artifacts/problem-brief-v[X.X].md` - Final approved Problem Brief

### Handoff Checklist
- [ ] Problem Brief saved to artifacts folder
- [ ] Version number reflects iteration count (e.g., v0.3 means 3rd iteration)
- [ ] All sections complete (no [TBD] placeholders)
- [ ] Stakeholder approval documented (date and any conditions)

### Context for Agent 2
Include when invoking Agent 2:
```
Problem Brief: [path to problem-brief-v0.X.md]

Additional context for competitive research:
- Known competitors: [list any mentioned by stakeholder]
- Reference products: [products stakeholder admires, even if different domain]
- Anti-references: [products stakeholder explicitly dislikes]
- Market segment: [any specific industry/vertical focus]

Constraints relevant to competition:
- Price ceiling: [what would target users pay?]
- Technical moat: [any unique technical capabilities?]
- Distribution advantage: [any existing audience or channel?]
```

### What Agent 2 Needs to Succeed
- Clear target user to evaluate competitors against
- Specific pain points to identify gaps in existing solutions
- Constraints to assess feasibility of differentiation strategies
- Success criteria to evaluate wedge strategy viability

## When to Invoke

- At project inception
- When pivoting or reframing
- When adding a major new feature area
- When user feedback suggests misalignment

## Example Usage

**Input:**
```
I want to build a tool to help researchers manage their literature reviews.
```

**Expected Output:**
Agent will ask 8-10 clarifying questions, then provide 3 problem framings (narrow, balanced, broad), then produce a complete Problem Brief.

## Quality Checklist

- [ ] Problem statement is specific and falsifiable
- [ ] At least 2 distinct user personas with real context
- [ ] 3-5 concrete jobs-to-be-done
- [ ] Success criteria are measurable (specific numbers + timeframes)
- [ ] Constraints are explicit
- [ ] "Out of scope" prevents feature creep with clear reasoning
- [ ] Stakeholder has explicitly approved the brief

## Output File

Save as: `artifacts/problem-brief-v0.1.md`
